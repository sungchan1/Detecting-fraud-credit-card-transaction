{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit_fraud.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgPSBObxenCF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSrRpPDp98Kx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Credit_fraud/data/creditcard.csv')\n",
        "std_scaler = StandardScaler()\n",
        "rob_scaler = RobustScaler()\n",
        "\n",
        "\n",
        "df['Time'] = std_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "df['Amount'] = std_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPxfF96LrBBL"
      },
      "source": [
        "# data distribution 시각화\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "row_cnt = len(df)\n",
        "zero_cnt = df['Class'].value_counts()[0]\n",
        "one_cnt = df['Class'].value_counts()[1]\n",
        "ratio_no_fraud = round(zero_cnt/row_cnt * 100,2)\n",
        "ratio_fraud = round(one_cnt/row_cnt * 100,2)\n",
        "\n",
        "\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "sns.countplot('Class', data=df, palette=colors)\n",
        "plt.title(f'Class Distributions \\n (0: No Fraud ({zero_cnt}, {ratio_no_fraud} %) '\n",
        "          f'\\n (1: Fraud ({one_cnt}, {ratio_fraud} %))', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNZFF6eqN3-M"
      },
      "source": [
        "# Data Correlation Matrices\n",
        "\n",
        "# pandas corr를 통해 피어슨 상관계수 사용\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(24,10))\n",
        "ax=sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
        "ax.set_title(\"Correlation Matrix\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnpvC8vxvWvS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
        "\n",
        "# UnderSampling 하기 전에 original dataframe을 test와 train으로 나눈다.\n",
        "# Under 혹은 OverSampling 이후 원래 데이터로 검증하기 위해서이다.\n",
        "\n",
        "x = df.drop('Class',axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# # random_state : 난수 값을 지정하면 여러번 다시 수행해도 동일한 결과가 나오게 해줌\n",
        "# # shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지 결정\n",
        "# # test : train = 4 : 1\n",
        "# sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "# for train_index, test_index in sss.split(X, y):\n",
        "#     original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
        "#     original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "# # 배열로 바꾸기\n",
        "# original_Xtrain = original_Xtrain.values\n",
        "# original_Xtest = original_Xtest.values\n",
        "# original_ytrain = original_ytrain.values\n",
        "# original_ytest = original_ytest.values\n",
        "\n",
        "# stratify 옵션은 train data와 test data의 샘플의 클래스 비율을 일정하게 하게한다.\n",
        "# train: test = 3 : 1\n",
        "original_xtrain, original_ytrain, original_xtest, original_ytest = train_test_split(x,y,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWANksx4KtES"
      },
      "source": [
        "# UnderSampling : NearMiss algorithm\n",
        "\n",
        "# dataframe.sample(frac=1) : data를 뽑기전 random하게 섞기\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# fraud의 수가 492개 이므로 492개의 non_fraud를 가져온다.\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "# pd.concat: data frame 합치기\n",
        "undersampling_df = pd.concat([fraud_df, non_fraud_df]).sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTSGOD0uNh6g"
      },
      "source": [
        "# undersampling data distribution 시각화\n",
        "\n",
        "\n",
        "u_zero_cnt = undersampling_df['Class'].value_counts()[0]\n",
        "u_one_cnt = undersampling_df['Class'].value_counts()[1]\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "sns.countplot('Class', data=undersampling_df, palette=colors)\n",
        "plt.title(f'Equally Distributed Classes \\n (0: No Fraud ({u_zero_cnt})'\n",
        "            f'\\n (1: Fraud ({u_one_cnt})', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd_N04uUPXy4"
      },
      "source": [
        "# undersampling data correlation\n",
        "# pandas corr를 통해 피어슨 상관계수 사용\n",
        "plt.figure(figsize=(24,10))\n",
        "\n",
        "sub_sample_corr = undersampling_df.corr()\n",
        "ax=sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
        "ax.set_title('UnderSampling Correlation Matrix \\n (use for reference)', fontsize=14)\n",
        "plt.show()\n",
        "# heatmap의 class를 보자.\n",
        "# V3, V10, V12, V14은 음의 상관계수를 가진다. 즉, 이 값들이 작을수록 fraud인 것이다.\n",
        "# 반대로 V2, V4, V11는 양의 상관계수를 가진다. 즉, 이 값들이 클수록 fraud인 것이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YBXwpkrZ2ud"
      },
      "source": [
        "# v3, v10, v12, v14 outlier 확인하기\n",
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V3\", data=undersampling_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V3 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=undersampling_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=undersampling_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=undersampling_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAy4hFi7adDb"
      },
      "source": [
        "# v2, v4, v11 outlier 확인하기\n",
        "f, axes = plt.subplots(ncols=3, figsize=(20,4))\n",
        "\n",
        "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V2\", data=undersampling_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V2 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V4\", data=undersampling_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V4 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V11\", data=undersampling_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V11 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF9IIpkRUZ7s"
      },
      "source": [
        "# V3, V10, V12, V14 data의 분포와 정규분포\n",
        "# V14만 정규분포 형태를 띄고 있다.\n",
        "from scipy.stats import norm\n",
        "\n",
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(20, 6))\n",
        "\n",
        "v3_fraud_dist = undersampling_df['V3'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v3_fraud_dist,ax=ax4, fit=norm, color='#C5B3F9')\n",
        "ax1.set_title('V3 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v10_fraud_dist = undersampling_df['V10'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v10_fraud_dist,ax=ax1, fit=norm, color='#C5B3F9')\n",
        "ax2.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v12_fraud_dist = undersampling_df['V12'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax3.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v14_fraud_dist = undersampling_df['V14'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v14_fraud_dist,ax=ax3, fit=norm, color='#FB8861')\n",
        "ax4.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuWXBvk1bYtz"
      },
      "source": [
        "# V2, V4, V11, V19 data의 분포와 정규분포\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
        "\n",
        "v2_fraud_dist = undersampling_df['V2'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v2_fraud_dist,ax=ax1, fit=norm, color='#C5B3F9')\n",
        "ax1.set_title('V2 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v4_fraud_dist = undersampling_df['V4'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v4_fraud_dist,ax=ax3, fit=norm, color='#FB8861')\n",
        "ax3.set_title('V4 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v11_fraud_dist = undersampling_df['V11'].loc[undersampling_df['Class'] == 1].values\n",
        "sns.distplot(v11_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('V11 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HItmU8hkXGLa"
      },
      "source": [
        "# Anomaly Detection: remove \"extreme outliers\"\n",
        "# Interquartile Range 방법을 사용해 25% 아래이거나 75% 위에 있는 data를 제거한다.\n",
        "\n",
        "# V2 outliers 제거\n",
        "v2_fraud = undersampling_df['V2']\n",
        "q25, q75 = np.percentile(v2_fraud, 25), np.percentile(v2_fraud, 75)\n",
        "v2_iqr = q75 - q25\n",
        "v2_cut_off = v2_iqr * 1.5\n",
        "v2_lower, v2_upper = q25 - v2_cut_off, q75 + v2_cut_off\n",
        "outliers = [x for x in v2_fraud if x < v2_lower or x > v2_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V2'] > v2_upper) | (undersampling_df['V2'] < v2_lower)].index)\n",
        "\n",
        "# v11 outliers 제거\n",
        "v11_fraud = undersampling_df['V11']\n",
        "q25, q75 = np.percentile(v11_fraud, 25), np.percentile(v11_fraud, 75)\n",
        "v11_iqr = q75 - q25\n",
        "v11_cut_off = v11_iqr * 1.5\n",
        "v11_lower, v11_upper = q25 - v11_cut_off, q75 + v11_cut_off\n",
        "outliers = [x for x in v11_fraud if x < v11_lower or x > v11_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V11'] > v11_upper) | (undersampling_df['V11'] < v11_lower)].index)\n",
        "\n",
        "# V3 outliers 제거\n",
        "v3_fraud = undersampling_df['V3']\n",
        "q25, q75 = np.percentile(v3_fraud, 25), np.percentile(v3_fraud, 75)\n",
        "v3_iqr = q75 - q25\n",
        "v3_cut_off = v3_iqr * 1.5\n",
        "v3_lower, v3_upper = q25 - v3_cut_off, q75 + v3_cut_off\n",
        "outliers = [x for x in v3_fraud if x < v3_lower or x > v3_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V3'] > v3_upper) | (undersampling_df['V3'] < v3_lower)].index)\n",
        "\n",
        "# V10 outliers 제거\n",
        "v10_fraud = undersampling_df['V10']\n",
        "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
        "v10_iqr = q75 - q25\n",
        "v10_cut_off = v10_iqr * 1.5\n",
        "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
        "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V10'] > v10_upper) | (undersampling_df['V10'] < v10_lower)].index)\n",
        "\n",
        "# V12 outliers 제거\n",
        "v12_fraud = undersampling_df['V12'].loc[undersampling_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
        "v12_iqr = q75 - q25\n",
        "v12_cut_off = v12_iqr * 1.5\n",
        "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
        "outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V12'] > v12_upper) | (undersampling_df['V12'] < v12_lower)].index)\n",
        "\n",
        "# v14 outliers 제거\n",
        "v14_fraud = undersampling_df['V14'].loc[undersampling_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
        "v14_iqr = q75 - q25\n",
        "v14_cut_off = v14_iqr * 1.5\n",
        "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
        "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
        "undersampling_df = undersampling_df.drop(undersampling_df[(undersampling_df['V14'] > v14_upper) | (undersampling_df['V14'] < v14_lower)].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxyxhiZdXlL"
      },
      "source": [
        "f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,8))\n",
        "\n",
        "# Feature V3\n",
        "sns.boxplot(x=\"Class\", y=\"V3\", data=undersampling_df, ax=ax1, palette=colors)\n",
        "ax1.set_title(\"V3 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "# Feature V10\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=undersampling_df, ax=ax2, palette=colors)\n",
        "ax2.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "# Feature V12\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=undersampling_df, ax=ax3, palette=colors)\n",
        "ax3.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "# Feature V14\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=undersampling_df,ax=ax4, palette=colors)\n",
        "ax4.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDJHhaUg7wmJ"
      },
      "source": [
        "f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,8))\n",
        "# Feature V2\n",
        "sns.boxplot(x=\"Class\", y=\"V2\", data=undersampling_df,ax=ax1, palette=colors)\n",
        "ax1.set_title(\"V2 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "# Feature V11\n",
        "sns.boxplot(x=\"Class\", y=\"V11\", data=undersampling_df,ax=ax2, palette=colors)\n",
        "ax2.set_title(\"V11 Feature \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "# Feature V19\n",
        "sns.boxplot(x=\"Class\", y=\"V19\", data=undersampling_df,ax=ax3, palette=colors)\n",
        "ax3.set_title(\"V19 Feature \\n Reduction of outliers\", fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZj4---1cUP_"
      },
      "source": [
        "# 차원 줄이기: t-sne, pca, truncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "# x_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(x.values)\n",
        "x_reduced_pca = PCA(n_components=10, random_state=42).fit_transform(x.values)\n",
        "# x_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(x.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2_d0swnh690"
      },
      "source": [
        "# 시각화\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
        "# labels = ['No Fraud', 'Fraud']\n",
        "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
        "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
        "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
        "\n",
        "# t-sne\n",
        "# ax1.scatter(x_reduced_tsne[:,0], x_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "# ax1.scatter(x_reduced_tsne[:,0], x_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "# ax1.set_title('t-SNE', fontsize=14)\n",
        "# ax1.grid(True)\n",
        "# ax1.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# pca\n",
        "ax2.scatter(x_reduced_pca[:,0], x_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "ax2.scatter(x_reduced_pca[:,0], x_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "ax2.set_title('PCA', fontsize=14)\n",
        "ax2.grid(True)\n",
        "ax2.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# truncatedSVD\n",
        "# ax3.scatter(x_reduced_svd[:,0], x_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
        "# ax3.scatter(x_reduced_svd[:,0], x_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
        "# ax3.set_title('Truncated SVD', fontsize=14)\n",
        "# ax3.grid(True)\n",
        "# ax3.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73tODfm2Vbrs"
      },
      "source": [
        "# 계층별 K-겹 교차 검증으로 KNN hyper parameter 구하기.\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "undersampling_df = undersampling_df.sample(frac=1)\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "# random_state : 난수 값을 지정하면 여러번 다시 수행해도 동일한 결과가 나오게 해줌\n",
        "# shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지 결정\n",
        "# test : train = 4 : 1\n",
        "num_split = 5\n",
        "sss = StratifiedKFold(n_splits=num_split, random_state=None, shuffle=False)\n",
        "\n",
        "param_size = 10\n",
        "param = [i for i in range(1,param_size+1)]\n",
        "\n",
        "Total_RC = [0] * num_split\n",
        "\n",
        "best_rc_param = [0] * num_split\n",
        "best_rc_score = [0] * num_split\n",
        "\n",
        "predict_rc = [[0] for i in range(num_split)]\n",
        "\n",
        "cnt = 0\n",
        "plt.figure(figsize=(12,40))\n",
        "\n",
        "for train_index, test_index in sss.split(x, y):\n",
        "    x_trainval, x_test = x.iloc[train_index], x.iloc[test_index]\n",
        "    y_trainval, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    # test_size: test 데이터 셋 비율, default = 0.25, random_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)\n",
        "    # train set에서 valid set을 따로 저장하며 최적의 hyper parameter 값을 찾는다.\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_trainval, y_trainval, test_size = 0.2)\n",
        "    \n",
        "    best_parameter = 0\n",
        "    best_recall_score = 0\n",
        "\n",
        "    RC = [0] * param_size\n",
        "\n",
        "    for i in param:\n",
        "        knc = KNeighborsClassifier(n_neighbors=i)\n",
        "        knc.fit(x_train, y_train)\n",
        "        y_pred = knc.predict(x_valid)\n",
        "\n",
        "\n",
        "        tmp_recall_score = recall_score(y_valid, y_pred)\n",
        "        RC[i-1] = tmp_recall_score\n",
        "        if best_recall_score < tmp_recall_score:\n",
        "            best_parameter = i  \n",
        "            best_recall_score = tmp_recall_score\n",
        "        \n",
        "    Total_RC[cnt] = RC\n",
        "    plt.subplot(5,1,cnt+1)\n",
        "    plt.plot(param,RC)\n",
        "    plt.xlabel('K', fontsize = 15)\n",
        "    plt.ylabel('Values of Recall', fontsize = 15)\n",
        "    plt.title(f'Undersampling KNN Recall #{cnt+1}', fontsize = 30)\n",
        "\n",
        "    best_rc_param[cnt] = best_parameter\n",
        "    cnt += 1\n",
        "    \n",
        "\n",
        "plt.show()\n",
        "\n",
        "Mean_RC = [[[0] for i in range(num_split)] for j in range(param_size)]\n",
        "\n",
        "for i in range(num_split):\n",
        "    for j in range(param_size):\n",
        "        Mean_RC[j][i] = Total_RC[i][j]\n",
        "\n",
        "for i in range(param_size):\n",
        "    print(f\"Mean of parameter {i} : {sum(Mean_RC[i])/num_split}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6_S1_HxvVoX"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "score = ['recall', 'precision', 'f1', 'accuracy']\n",
        "\n",
        "knc = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "for s in score:\n",
        "    print(f\"{s}:  {cross_val_score(knc,x,y,scoring=s,cv = 5).mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVQ5WkIdeztJ"
      },
      "source": [
        "# 계층별 k-겹 교차 검증으로 LogisticRegression hyper parameter 구하기\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "x = undersampling_df.drop('Class',axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "num_split = 5\n",
        "sss = StratifiedKFold(n_splits=num_split, random_state=None, shuffle=False)\n",
        "\n",
        "param_size = 7\n",
        "regular_c = [0.001, 0.01, 0.1, 1, 10, 100, 1000 ]\n",
        "\n",
        "Total_RC = [0] * num_split\n",
        "\n",
        "best_rc_param = [0] * num_split\n",
        "best_rc_score = [0] * num_split\n",
        "\n",
        "predict_rc = [0] * num_split\n",
        "\n",
        "cnt = 0\n",
        "plt.figure(figsize=(12,40))\n",
        "\n",
        "for train_index, test_index in sss.split(x, y):\n",
        "    x_trainval, x_test = x.iloc[train_index], x.iloc[test_index]\n",
        "    y_trainval, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_trainval, y_trainval, test_size = 0.2)\n",
        "\n",
        "    # solver : 작은 데이터일 땐 liblinear이 좋다.\n",
        "    # c: 정규화 제약 조건\n",
        "    # liblinear는 peanlty가 none이면 안 되고, L1, L2를 지원한다. default = L2\n",
        "    # logisticregression은 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제한다.\n",
        "    # logisticregression에서 규제를 제어하는 매개변수는 c이다.\n",
        "    # c는 alpha와 반대로 작을수록 규제가 커진다.\n",
        "    # c의 기본값은 1이다.\n",
        "\n",
        "    best_parameter = 0\n",
        "    best_recall_score = 0\n",
        "\n",
        "    RC = [0] * param_size\n",
        "\n",
        "    for i in range(param_size):\n",
        "        c = regular_c[i]\n",
        "        # lr = LogisticRegression(solver='liblinear', C=c, max_iter=10000, penalty = 'l1')\n",
        "        lr = LogisticRegression(C=c, max_iter=1000)\n",
        "        lr.fit(x_train, y_train)\n",
        "        y_pred = lr.predict(x_valid)\n",
        "\n",
        "        tmp_recall_score = recall_score(y_valid, y_pred)\n",
        "        RC[i-1] = tmp_recall_score\n",
        "        if best_recall_score < tmp_recall_score:\n",
        "            best_parameter = c  \n",
        "            best_recall_score = tmp_recall_score\n",
        "        \n",
        "    Total_RC[cnt] = RC\n",
        "    plt.subplot(5,1,cnt+1)\n",
        "    plt.plot(np.log10(regular_c),RC)\n",
        "    plt.xlabel('C', fontsize = 15)\n",
        "    plt.ylabel('Values of Recall', fontsize = 15)\n",
        "    plt.title(f'Undersampling LogisticRegression Recall #{cnt+1}', fontsize = 30)\n",
        "\n",
        "    best_rc_param[cnt] = best_parameter\n",
        "    cnt += 1\n",
        "    \n",
        "\n",
        "plt.show()\n",
        "\n",
        "Mean_RC = [[[0] for i in range(num_split)] for j in range(param_size)]\n",
        "\n",
        "for i in range(num_split):\n",
        "    for j in range(param_size):\n",
        "        Mean_RC[j][i] = Total_RC[i][j]\n",
        "\n",
        "for i in range(param_size):\n",
        "    print(f\"Mean of parameter {regular_c[i]} : {sum(Mean_RC[i])/num_split}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfqxuKaSTZ_A"
      },
      "source": [
        "# Recall : TP/TP+FN\n",
        "# Precision = TP/TP+FP\n",
        "# Accuracy = \n",
        "# F1 score = (TP+TN)/(TP+FP+FN+TN)\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "score = ['recall', 'precision', 'f1', 'accuracy']\n",
        "\n",
        "lr = LogisticRegression(C=1, max_iter=1000)\n",
        "\n",
        "for s in score:\n",
        "    print(f\"{s}:  {cross_val_score(lr,x,y,scoring=s,cv = 5).mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-gD32QgVDc"
      },
      "source": [
        "# SVM hyper parameter 구하기\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "svc_params = {'C': [0.1, 0.5, 0.7, 0.9, 1, 5, 7, 9, 10], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "grid_svc = GridSearchCV(SVC(), svc_params, scoring='recall')\n",
        "grid_svc.fit(x_train, y_train)\n",
        "\n",
        "# SVC best estimator\n",
        "svc = grid_svc.best_estimator_\n",
        "\n",
        "print(svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBmqZwcSr-dL"
      },
      "source": [
        "# Recall : TP/TP+FN\n",
        "# Precision = TP/TP+FP\n",
        "# Accuracy = \n",
        "# F1 score = (TP+TN)/(TP+FP+FN+TN)\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "x = undersampling_df.drop('Class', axis=1)\n",
        "y = undersampling_df['Class']\n",
        "\n",
        "score = ['recall', 'precision', 'f1', 'accuracy']\n",
        "\n",
        "svc = SVC(C=0.7, kernel='linear')\n",
        "\n",
        "for s in score:\n",
        "    print(f\"{s} Score:  {cross_val_score(svc,x,y,scoring=s,cv = 5).mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmK1Jh1HrZ5V"
      },
      "source": [
        "# 계층별 k-겹 교차 검증으로 앙상블 tree parameter 구하기"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}