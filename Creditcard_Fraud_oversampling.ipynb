{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creditcard Fraud oversampling.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNrxqYTgL7CBVJCAvuqZwE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungchan1/goingSaboho/blob/gwangseok/Creditcard_Fraud_oversampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_01bcXc_OXy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8JdIvcA_ayx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, ShuffleSplit, learning_curve, cross_val_score\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Credit_fraud/data/creditcard.csv')\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "\n",
        "df['Time'] = std_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "df['Amount'] = std_scaler.fit_transform(df['Amount'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeAeS1lwUmLF"
      },
      "source": [
        "x = df.drop('Class',axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# stratify 옵션은 train data와 test data의 샘플의 클래스 비율을 일정하게 하게한다.\n",
        "# train: test = 4 : 1\n",
        "# train: 394개의 fraud // test: 98개의 fraud\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tv77EOWUqnC"
      },
      "source": [
        "# Oversampling SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=0)\n",
        "x_train_over, y_train_over = smote.fit_sample(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSYUKaf_UvhB"
      },
      "source": [
        "# Recall : TP/TP+FN\n",
        "# Precision = TP/TP+FP\n",
        "# Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "# F1 score = 2 * (Precison * Recall) / (Precision + Recall)\n",
        "def print_metric(y_test, y_pred):\n",
        "    print(f\"Recall Score: {recall_score(y_test, y_pred)}\")\n",
        "    print(f\"Precision Score: {precision_score(y_test, y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
        "    print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"AUC: {roc_auc_score(y_test, y_pred, average='macro')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSZPuReU2GA"
      },
      "source": [
        "# ylim: y축 범위 제한\n",
        "# cv: default = none >> 5\n",
        "# n_jobs: 연산을 위한 CPU 개수 지정\n",
        "# train_sizes: learning curve 생성시 사용할 데이터 사이즈 지정 (training set의 상대적인 또는 절대적인 숫자)\n",
        "def plot_learning_curve(estimator, x, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), s=None):\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    # train_sizes : (392 + 392)의 80% 를 0.1, 0.325, 0.55, 0.775, 1의 비율로 학습시긴다.\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=s)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    # 평균에 표준 편차를 +-해준 영역을 색칠한다.\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n",
        "    plt.xlabel('Training size')\n",
        "    plt.ylabel('F1 Score')\n",
        "    # 그림에 선 표시\n",
        "    plt.grid(True)\n",
        "    # 범례 표시: best - 자동으로 최적의 위치에\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "cv = ShuffleSplit(n_splits=5, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7sMNBbC_bd3"
      },
      "source": [
        "# data distribution 시각화\n",
        "row_cnt = len(df)\n",
        "zero_cnt = df['Class'].value_counts()[0]\n",
        "one_cnt = df['Class'].value_counts()[1]\n",
        "ratio_no_fraud = round(zero_cnt/row_cnt * 100,2)\n",
        "ratio_fraud = round(one_cnt/row_cnt * 100,2)\n",
        "\n",
        "\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "sns.countplot('Class', data=df, palette=colors)\n",
        "plt.title(f'Class Distributions \\n (0: No Fraud ({zero_cnt}, {ratio_no_fraud} %) '\n",
        "          f'\\n (1: Fraud ({one_cnt}, {ratio_fraud} %))', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewacAIw6_fnr"
      },
      "source": [
        "# Data Correlation Matrices\n",
        "# pandas corr를 통해 피어슨 상관계수 사용\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(24,10))\n",
        "ax=sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
        "ax.set_title(\"Correlation Matrix\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw-XhhRe_tYw"
      },
      "source": [
        "new_df = pd.concat([pd.DataFrame(x_train_over), pd.DataFrame(y_train_over)], axis=1)\n",
        "label = new_df.iloc[:,-1]\n",
        "u_zero_cnt = label.value_counts()[0]\n",
        "u_one_cnt = label.value_counts()[1]\n",
        "colors = [\"#0101DF\", \"#DF0101\"]\n",
        "sns.countplot(label, palette=colors)\n",
        "plt.title(f'Equally Distributed Classes \\n (0: No Fraud ({u_zero_cnt})'\n",
        "            f'\\n (1: Fraud ({u_one_cnt})', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlGpIKZyDAbM"
      },
      "source": [
        "# pandas corr를 통해 피어슨 상관계수 사용\n",
        "plt.figure(figsize=(24,10))\n",
        "\n",
        "sub_sample_corr = new_df.corr()\n",
        "ax=sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20})\n",
        "ax.set_title('UnderSampling Correlation Matrix \\n (use for reference)', fontsize=14)\n",
        "plt.show()\n",
        "# heatmap의 class를 보자.\n",
        "# V3, V10, V12, V14은 음의 상관계수를 가진다. 즉, 이 값들이 작을수록 fraud인 것이다.\n",
        "# 반대로 V2, V4, V11는 양의 상관계수를 가진다. 즉, 이 값들이 클수록 fraud인 것이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcsLa0sYDGdC"
      },
      "source": [
        "# GridSearch + Cross Validation으로 KNN hyper parameter 구하기.\n",
        "\n",
        "knc_params = {\"n_neighbors\": list(range(1,11)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "\n",
        "# GridSearchCV 인자설명\n",
        "# cv = 하나의 파라미터 쌍으로 모델링할 때 train, test 교차검증을 3번실시하겠다는 뜻\n",
        "# refit=True : GridSearch한 후 가장 최고로 좋은 파라미터로 학습시켜 놓겠다.\n",
        "# 이것 때문에 애초에 GridSearchCV 적용한 객체만으로 최적의 파라미터 적용된 모델로드 가능\n",
        "\n",
        "grid_knc = GridSearchCV(KNeighborsClassifier(), knc_params, cv = 5, refit=True, scoring='f1')\n",
        "\n",
        "# GridSearch 하면서 모든 파라미터값들에 대해 학습 수행\n",
        "grid_knc.fit(x_train_over, y_train_over)\n",
        "\n",
        "\n",
        "# 최적의 파라미터는 best_params_에 할당되어있음\n",
        "print(f\"최적의 파라미터 : {grid_knc.best_params_}\")\n",
        "print(f\"최적의 파라미터 모델의 F1 : {grid_knc.best_score_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojwLhXSODqOI"
      },
      "source": [
        "# 최적의 파라미터로 학습되어 있는 모델링 할당\n",
        "# knc = grid_knc.best_estimator_\n",
        "\n",
        "# n이 증가할수록 precision 값은 증가하지만 recall 값은 감소한다.\n",
        "knc = KNeighborsClassifier(n_neighbors=3)\n",
        "knc.fit(x_train_over, y_train_over)\n",
        "y_pred = knc.predict(x_test)\n",
        "\n",
        "print_metric(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-M0ek1UD790"
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"KNN Learning Curve\", fontsize=14)\n",
        "plot_learning_curve(KNeighborsClassifier(n_neighbors=3), x_train_over, y_train_over , (0.80, 1.01), cv=cv, n_jobs=-1, s='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM26fATqELn3"
      },
      "source": [
        "# LogisticRegression은 규제를 강하게 줘 overfitting을 방지해야 한다. >> 0.000001 정도\n",
        "lr = LogisticRegression(C=0.000001)\n",
        "lr.fit(x_train_over, y_train_over)\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "print_metric(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8-weGr9EdJY"
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
        "plot_learning_curve(lr, x_train_over, y_train_over, (0.80, 1.01), cv=cv, s='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6dLvnIjEmrP"
      },
      "source": [
        "svc = SVC(C=0.0001, kernel='linear')\n",
        "svc.fit(x_train_over, y_train_over)\n",
        "y_pred = svc.predict(x_test)\n",
        "\n",
        "print_metric(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN0L2ntLEpYR"
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Support Vector Machine Learning Curve\", fontsize=14)\n",
        "plot_learning_curve(svc, x_train_over, y_train_over, (0.80, 1.01), cv=cv, s='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct1Tkpl5ErWd"
      },
      "source": [
        "lgb = LGBMClassifier(n_estimators=1000,num_leaves=64,n_jobs=-1,boost_from_average=False, application='binary')\n",
        "lgb.fit(x_train_over, y_train_over)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmeP6ZFWFZRU"
      },
      "source": [
        "y_pred = lgb.predict(x_test)\n",
        "print_metric(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99BXbzTFnR_"
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"LightGBM Learning Curve\", fontsize=14)\n",
        "plot_learning_curve(lgb, x_train_over, y_train_over, (0.80, 1.01), cv=cv, s='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RouhYgRrEIRg"
      },
      "source": [
        "x_train_over, y_train_over"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMvgHTbKT7ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorlow.keras.models import Sequential, load_model\n",
        "from tensorlow.keras.layers import Dense, Flatten, Dropout, InputLayer\n",
        "from tensorlow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorlow.normalization import BatchNormalization\n",
        "from tensorlow.keras.metrics import FalseNegatives, FalsePositives, TrueNegatives\n",
        "from tensorlow.keras.metrics import TruePositives, Precision, Recall, Accuracy\n",
        "\n",
        "metrics = [\n",
        "    FalseNegatives(name=\"fn\"),\n",
        "    FalsePositives(name=\"fp\"),\n",
        "    TrueNegatives(name=\"tn\"),\n",
        "    TruePositives(name=\"tp\"),\n",
        "    Precision(name=\"precision\"),\n",
        "    Recall(name=\"recall\"),\n",
        "    Accuracy(name='accuracy')\n",
        "]\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(256, activation=\"relu\", input_shape=(train_features.shape[-1],)),\n",
        "        Dense(256, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}